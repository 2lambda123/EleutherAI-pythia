{
  "results": {
    "crows_pairs_english_physical_appearance": {
      "likelihood_difference": 2.993923611111111,
      "likelihood_difference_stderr": 0.27932297068840983,
      "pct_stereotype": 0.6527777777777778,
      "pct_stereotype_stderr": 0.056501146768529645
    },
    "crows_pairs_english_religion": {
      "likelihood_difference": 3.355574324324324,
      "likelihood_difference_stderr": 0.33027533961060374,
      "pct_stereotype": 0.7387387387387387,
      "pct_stereotype_stderr": 0.04188770861432398
    },
    "crows_pairs_french": {
      "likelihood_difference": 3.6026945438282647,
      "likelihood_difference_stderr": 0.09017899964773156,
      "pct_stereotype": 0.4555754323196184,
      "pct_stereotype_stderr": 0.012164996779720268
    },
    "crows_pairs_english_race_color": {
      "likelihood_difference": 3.28248031496063,
      "likelihood_difference_stderr": 0.15416979652114668,
      "pct_stereotype": 0.5275590551181102,
      "pct_stereotype_stderr": 0.022172023280100765
    },
    "crows_pairs_english_age": {
      "likelihood_difference": 3.433379120879121,
      "likelihood_difference_stderr": 0.33342997711548733,
      "pct_stereotype": 0.6043956043956044,
      "pct_stereotype_stderr": 0.05154303032773001
    },
    "crows_pairs_english_autre": {
      "likelihood_difference": 4.099431818181818,
      "likelihood_difference_stderr": 1.6453426121835362,
      "pct_stereotype": 0.6363636363636364,
      "pct_stereotype_stderr": 0.15212000482437738
    },
    "crows_pairs_french_physical_appearance": {
      "likelihood_difference": 4.010416666666667,
      "likelihood_difference_stderr": 0.5241782992478669,
      "pct_stereotype": 0.5694444444444444,
      "pct_stereotype_stderr": 0.058763966770846124
    },
    "crows_pairs_english_disability": {
      "likelihood_difference": 5.469230769230769,
      "likelihood_difference_stderr": 0.5400786010867249,
      "pct_stereotype": 0.7384615384615385,
      "pct_stereotype_stderr": 0.05493406483494501
    },
    "crows_pairs_french_socioeconomic": {
      "likelihood_difference": 3.4470663265306123,
      "likelihood_difference_stderr": 0.26931558994395244,
      "pct_stereotype": 0.5714285714285714,
      "pct_stereotype_stderr": 0.035438495596916704
    },
    "crows_pairs_english": {
      "likelihood_difference": 3.3974545319022065,
      "likelihood_difference_stderr": 0.08354097164222715,
      "pct_stereotype": 0.6100178890876565,
      "pct_stereotype_stderr": 0.01191397326663218
    },
    "crows_pairs_english_gender": {
      "likelihood_difference": 2.743359375,
      "likelihood_difference_stderr": 0.1522153730382269,
      "pct_stereotype": 0.63125,
      "pct_stereotype_stderr": 0.02701290980694683
    },
    "crows_pairs_french_sexual_orientation": {
      "likelihood_difference": 3.6878434065934065,
      "likelihood_difference_stderr": 0.36338220986084463,
      "pct_stereotype": 0.7362637362637363,
      "pct_stereotype_stderr": 0.04644942852497395
    },
    "crows_pairs_english_sexual_orientation": {
      "likelihood_difference": 4.560483870967742,
      "likelihood_difference_stderr": 0.4602417623928664,
      "pct_stereotype": 0.8602150537634409,
      "pct_stereotype_stderr": 0.036152622588464155
    },
    "crows_pairs_french_age": {
      "likelihood_difference": 2.967361111111111,
      "likelihood_difference_stderr": 0.3040440337186971,
      "pct_stereotype": 0.5,
      "pct_stereotype_stderr": 0.052999894000318
    },
    "crows_pairs_english_socioeconomic": {
      "likelihood_difference": 3.791282894736842,
      "likelihood_difference_stderr": 0.23978537346281797,
      "pct_stereotype": 0.6157894736842106,
      "pct_stereotype_stderr": 0.03538097998767891
    },
    "crows_pairs_french_autre": {
      "likelihood_difference": 3.2475961538461537,
      "likelihood_difference_stderr": 0.9661582728123624,
      "pct_stereotype": 0.38461538461538464,
      "pct_stereotype_stderr": 0.1404416814115811
    },
    "crows_pairs_english_nationality": {
      "likelihood_difference": 3.271412037037037,
      "likelihood_difference_stderr": 0.23205084143092586,
      "pct_stereotype": 0.5416666666666666,
      "pct_stereotype_stderr": 0.033981108902946366
    },
    "crows_pairs_french_gender": {
      "likelihood_difference": 3.330510124610592,
      "likelihood_difference_stderr": 0.17615089045112176,
      "pct_stereotype": 0.4953271028037383,
      "pct_stereotype_stderr": 0.027949629024360143
    },
    "crows_pairs_french_race_color": {
      "likelihood_difference": 3.3444972826086956,
      "likelihood_difference_stderr": 0.170961416200705,
      "pct_stereotype": 0.3217391304347826,
      "pct_stereotype_stderr": 0.021804391953393117
    },
    "crows_pairs_french_disability": {
      "likelihood_difference": 4.426136363636363,
      "likelihood_difference_stderr": 0.47331144843511846,
      "pct_stereotype": 0.5757575757575758,
      "pct_stereotype_stderr": 0.06130137276858361
    },
    "crows_pairs_french_religion": {
      "likelihood_difference": 3.4953804347826085,
      "likelihood_difference_stderr": 0.3950240463930227,
      "pct_stereotype": 0.6347826086956522,
      "pct_stereotype_stderr": 0.04509577025262067
    },
    "crows_pairs_french_nationality": {
      "likelihood_difference": 4.469614624505929,
      "likelihood_difference_stderr": 0.25052124592483604,
      "pct_stereotype": 0.30039525691699603,
      "pct_stereotype_stderr": 0.028878367428103884
    }
  },
  "versions": {
    "crows_pairs_english_physical_appearance": 0,
    "crows_pairs_english_religion": 0,
    "crows_pairs_french": 0,
    "crows_pairs_english_race_color": 0,
    "crows_pairs_english_age": 0,
    "crows_pairs_english_autre": 0,
    "crows_pairs_french_physical_appearance": 0,
    "crows_pairs_english_disability": 0,
    "crows_pairs_french_socioeconomic": 0,
    "crows_pairs_english": 0,
    "crows_pairs_english_gender": 0,
    "crows_pairs_french_sexual_orientation": 0,
    "crows_pairs_english_sexual_orientation": 0,
    "crows_pairs_french_age": 0,
    "crows_pairs_english_socioeconomic": 0,
    "crows_pairs_french_autre": 0,
    "crows_pairs_english_nationality": 0,
    "crows_pairs_french_gender": 0,
    "crows_pairs_french_race_color": 0,
    "crows_pairs_french_disability": 0,
    "crows_pairs_french_religion": 0,
    "crows_pairs_french_nationality": 0
  },
  "config": {
    "model": "hf-causal",
    "model_args": "pretrained=EleutherAI/pythia-1.4b-deduped,revision=step121000",
    "num_fewshot": 0,
    "batch_size": 4,
    "device": "cuda:3",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}