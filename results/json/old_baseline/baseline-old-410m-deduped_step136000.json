{
  "results": {
    "crows_pairs_english": {
      "likelihood_difference": 3.4931052474657127,
      "likelihood_difference_stderr": 0.08893953807559064,
      "pct_stereotype": 0.554561717352415,
      "pct_stereotype_stderr": 0.012140363921944515
    },
    "crows_pairs_french_gender": {
      "likelihood_difference": 3.4364291277258565,
      "likelihood_difference_stderr": 0.18165290803016443,
      "pct_stereotype": 0.4797507788161994,
      "pct_stereotype_stderr": 0.0279279188851323
    },
    "crows_pairs_french_religion": {
      "likelihood_difference": 3.3152173913043477,
      "likelihood_difference_stderr": 0.39229264834347055,
      "pct_stereotype": 0.41739130434782606,
      "pct_stereotype_stderr": 0.046185723795122625
    },
    "crows_pairs_english_socioeconomic": {
      "likelihood_difference": 3.817105263157895,
      "likelihood_difference_stderr": 0.2339942473530912,
      "pct_stereotype": 0.5947368421052631,
      "pct_stereotype_stderr": 0.03571084126496388
    },
    "crows_pairs_french_sexual_orientation": {
      "likelihood_difference": 3.714629120879121,
      "likelihood_difference_stderr": 0.3217437077350126,
      "pct_stereotype": 0.6923076923076923,
      "pct_stereotype_stderr": 0.04865042554105199
    },
    "crows_pairs_english_gender": {
      "likelihood_difference": 2.91396484375,
      "likelihood_difference_stderr": 0.18837147259180534,
      "pct_stereotype": 0.575,
      "pct_stereotype_stderr": 0.027677894260962167
    },
    "crows_pairs_english_race_color": {
      "likelihood_difference": 3.4272883858267718,
      "likelihood_difference_stderr": 0.16014100801680453,
      "pct_stereotype": 0.4507874015748031,
      "pct_stereotype_stderr": 0.022097958358675954
    },
    "crows_pairs_english_sexual_orientation": {
      "likelihood_difference": 4.165322580645161,
      "likelihood_difference_stderr": 0.44887458979231376,
      "pct_stereotype": 0.7526881720430108,
      "pct_stereotype_stderr": 0.0449817218566707
    },
    "crows_pairs_french_nationality": {
      "likelihood_difference": 4.663537549407114,
      "likelihood_difference_stderr": 0.2559471774209764,
      "pct_stereotype": 0.30434782608695654,
      "pct_stereotype_stderr": 0.02898550724637675
    },
    "crows_pairs_french_autre": {
      "likelihood_difference": 3.829326923076923,
      "likelihood_difference_stderr": 0.9926841729852275,
      "pct_stereotype": 0.3076923076923077,
      "pct_stereotype_stderr": 0.13323467750529824
    },
    "crows_pairs_french_socioeconomic": {
      "likelihood_difference": 3.862484056122449,
      "likelihood_difference_stderr": 0.28794863851965297,
      "pct_stereotype": 0.576530612244898,
      "pct_stereotype_stderr": 0.03538383416117806
    },
    "crows_pairs_english_nationality": {
      "likelihood_difference": 3.453269675925926,
      "likelihood_difference_stderr": 0.25883531479858707,
      "pct_stereotype": 0.4675925925925926,
      "pct_stereotype_stderr": 0.034028015813589656
    },
    "crows_pairs_french_age": {
      "likelihood_difference": 3.55,
      "likelihood_difference_stderr": 0.3511919299921846,
      "pct_stereotype": 0.4222222222222222,
      "pct_stereotype_stderr": 0.05235473399540656
    },
    "crows_pairs_english_age": {
      "likelihood_difference": 3.1758241758241756,
      "likelihood_difference_stderr": 0.3082946003415125,
      "pct_stereotype": 0.5824175824175825,
      "pct_stereotype_stderr": 0.051983687837675575
    },
    "crows_pairs_english_physical_appearance": {
      "likelihood_difference": 3.4446614583333335,
      "likelihood_difference_stderr": 0.329165038405574,
      "pct_stereotype": 0.6111111111111112,
      "pct_stereotype_stderr": 0.057855371034784615
    },
    "crows_pairs_english_autre": {
      "likelihood_difference": 5.3664772727272725,
      "likelihood_difference_stderr": 2.361839625226331,
      "pct_stereotype": 0.6363636363636364,
      "pct_stereotype_stderr": 0.15212000482437738
    },
    "crows_pairs_french": {
      "likelihood_difference": 3.862580128205128,
      "likelihood_difference_stderr": 0.09513007293216569,
      "pct_stereotype": 0.46630888491353606,
      "pct_stereotype_stderr": 0.012185541257180464
    },
    "crows_pairs_english_disability": {
      "likelihood_difference": 5.788461538461538,
      "likelihood_difference_stderr": 0.577444769884142,
      "pct_stereotype": 0.6461538461538462,
      "pct_stereotype_stderr": 0.05977027026123099
    },
    "crows_pairs_french_disability": {
      "likelihood_difference": 5.0293560606060606,
      "likelihood_difference_stderr": 0.4983221770739995,
      "pct_stereotype": 0.5454545454545454,
      "pct_stereotype_stderr": 0.061760565498796154
    },
    "crows_pairs_french_race_color": {
      "likelihood_difference": 3.7874320652173914,
      "likelihood_difference_stderr": 0.1981665262172282,
      "pct_stereotype": 0.45652173913043476,
      "pct_stereotype_stderr": 0.023249599562309698
    },
    "crows_pairs_english_religion": {
      "likelihood_difference": 3.1853885135135136,
      "likelihood_difference_stderr": 0.3261082580483362,
      "pct_stereotype": 0.7837837837837838,
      "pct_stereotype_stderr": 0.039250566187156444
    },
    "crows_pairs_french_physical_appearance": {
      "likelihood_difference": 3.8168402777777777,
      "likelihood_difference_stderr": 0.43746039669023484,
      "pct_stereotype": 0.5416666666666666,
      "pct_stereotype_stderr": 0.05913268547421811
    }
  },
  "versions": {
    "crows_pairs_english": 0,
    "crows_pairs_french_gender": 0,
    "crows_pairs_french_religion": 0,
    "crows_pairs_english_socioeconomic": 0,
    "crows_pairs_french_sexual_orientation": 0,
    "crows_pairs_english_gender": 0,
    "crows_pairs_english_race_color": 0,
    "crows_pairs_english_sexual_orientation": 0,
    "crows_pairs_french_nationality": 0,
    "crows_pairs_french_autre": 0,
    "crows_pairs_french_socioeconomic": 0,
    "crows_pairs_english_nationality": 0,
    "crows_pairs_french_age": 0,
    "crows_pairs_english_age": 0,
    "crows_pairs_english_physical_appearance": 0,
    "crows_pairs_english_autre": 0,
    "crows_pairs_french": 0,
    "crows_pairs_english_disability": 0,
    "crows_pairs_french_disability": 0,
    "crows_pairs_french_race_color": 0,
    "crows_pairs_english_religion": 0,
    "crows_pairs_french_physical_appearance": 0
  },
  "config": {
    "model": "hf-causal",
    "model_args": "pretrained=EleutherAI/pythia-410m-deduped,revision=step136000",
    "num_fewshot": 0,
    "batch_size": 8,
    "device": "cuda:2",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}