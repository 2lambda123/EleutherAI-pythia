{
  "results": {
    "crows_pairs_english_race_color": {
      "likelihood_difference": 3.374630905511811,
      "likelihood_difference_stderr": 0.1586269567748573,
      "pct_stereotype": 0.484251968503937,
      "pct_stereotype_stderr": 0.022194762762659328
    },
    "crows_pairs_french_age": {
      "likelihood_difference": 4.317013888888889,
      "likelihood_difference_stderr": 0.39213497517318546,
      "pct_stereotype": 0.45555555555555555,
      "pct_stereotype_stderr": 0.05279009646630345
    },
    "crows_pairs_french_sexual_orientation": {
      "likelihood_difference": 4.635302197802198,
      "likelihood_difference_stderr": 0.39264429384800603,
      "pct_stereotype": 0.6703296703296703,
      "pct_stereotype_stderr": 0.049552195085965874
    },
    "crows_pairs_english_socioeconomic": {
      "likelihood_difference": 3.7039473684210527,
      "likelihood_difference_stderr": 0.23178972303770254,
      "pct_stereotype": 0.6105263157894737,
      "pct_stereotype_stderr": 0.035469931637371596
    },
    "crows_pairs_french_autre": {
      "likelihood_difference": 5.043269230769231,
      "likelihood_difference_stderr": 1.677235887740968,
      "pct_stereotype": 0.6153846153846154,
      "pct_stereotype_stderr": 0.14044168141158106
    },
    "crows_pairs_english_autre": {
      "likelihood_difference": 4.275568181818182,
      "likelihood_difference_stderr": 1.7665301791837416,
      "pct_stereotype": 0.36363636363636365,
      "pct_stereotype_stderr": 0.15212000482437738
    },
    "crows_pairs_french_disability": {
      "likelihood_difference": 6.21780303030303,
      "likelihood_difference_stderr": 0.667089812931768,
      "pct_stereotype": 0.48484848484848486,
      "pct_stereotype_stderr": 0.06198888629778894
    },
    "crows_pairs_english_age": {
      "likelihood_difference": 2.7101648351648353,
      "likelihood_difference_stderr": 0.2887883164197369,
      "pct_stereotype": 0.5714285714285714,
      "pct_stereotype_stderr": 0.05216405309573015
    },
    "crows_pairs_french_socioeconomic": {
      "likelihood_difference": 5.03483737244898,
      "likelihood_difference_stderr": 0.3533714658527315,
      "pct_stereotype": 0.3520408163265306,
      "pct_stereotype_stderr": 0.0342021201896923
    },
    "crows_pairs_english_religion": {
      "likelihood_difference": 3.5563063063063063,
      "likelihood_difference_stderr": 0.43237736398452553,
      "pct_stereotype": 0.6396396396396397,
      "pct_stereotype_stderr": 0.04577621167070314
    },
    "crows_pairs_english_gender": {
      "likelihood_difference": 2.8662109375,
      "likelihood_difference_stderr": 0.23005079988720817,
      "pct_stereotype": 0.553125,
      "pct_stereotype_stderr": 0.027836160509246803
    },
    "crows_pairs_english_physical_appearance": {
      "likelihood_difference": 3.3812934027777777,
      "likelihood_difference_stderr": 0.3790238181501986,
      "pct_stereotype": 0.6111111111111112,
      "pct_stereotype_stderr": 0.057855371034784615
    },
    "crows_pairs_french_nationality": {
      "likelihood_difference": 6.8125,
      "likelihood_difference_stderr": 0.40368418179218646,
      "pct_stereotype": 0.2964426877470356,
      "pct_stereotype_stderr": 0.028768673758013917
    },
    "crows_pairs_english": {
      "likelihood_difference": 3.477843619558736,
      "likelihood_difference_stderr": 0.09687138962867986,
      "pct_stereotype": 0.5354800238521169,
      "pct_stereotype_stderr": 0.012182511427054798
    },
    "crows_pairs_french_physical_appearance": {
      "likelihood_difference": 5.555121527777778,
      "likelihood_difference_stderr": 0.7219568556960506,
      "pct_stereotype": 0.5416666666666666,
      "pct_stereotype_stderr": 0.05913268547421809
    },
    "crows_pairs_french_gender": {
      "likelihood_difference": 4.263531931464175,
      "likelihood_difference_stderr": 0.23718507055528726,
      "pct_stereotype": 0.46417445482866043,
      "pct_stereotype_stderr": 0.027879009258377076
    },
    "crows_pairs_french": {
      "likelihood_difference": 4.924092501490757,
      "likelihood_difference_stderr": 0.12560596860693737,
      "pct_stereotype": 0.4400715563506261,
      "pct_stereotype_stderr": 0.012125255739786706
    },
    "crows_pairs_english_disability": {
      "likelihood_difference": 5.0197115384615385,
      "likelihood_difference_stderr": 0.6232546126394937,
      "pct_stereotype": 0.5692307692307692,
      "pct_stereotype_stderr": 0.061897988228581086
    },
    "crows_pairs_french_religion": {
      "likelihood_difference": 4.347010869565217,
      "likelihood_difference_stderr": 0.5049499883890834,
      "pct_stereotype": 0.4434782608695652,
      "pct_stereotype_stderr": 0.04652911680416962
    },
    "crows_pairs_french_race_color": {
      "likelihood_difference": 4.331657608695652,
      "likelihood_difference_stderr": 0.22957692666728616,
      "pct_stereotype": 0.46304347826086956,
      "pct_stereotype_stderr": 0.023274164732827177
    },
    "crows_pairs_english_sexual_orientation": {
      "likelihood_difference": 4.810147849462366,
      "likelihood_difference_stderr": 0.6036762080382349,
      "pct_stereotype": 0.5913978494623656,
      "pct_stereotype_stderr": 0.051250284528759994
    },
    "crows_pairs_english_nationality": {
      "likelihood_difference": 3.6648582175925926,
      "likelihood_difference_stderr": 0.2650128502289413,
      "pct_stereotype": 0.4444444444444444,
      "pct_stereotype_stderr": 0.03388857118502325
    }
  },
  "versions": {
    "crows_pairs_english_race_color": 0,
    "crows_pairs_french_age": 0,
    "crows_pairs_french_sexual_orientation": 0,
    "crows_pairs_english_socioeconomic": 0,
    "crows_pairs_french_autre": 0,
    "crows_pairs_english_autre": 0,
    "crows_pairs_french_disability": 0,
    "crows_pairs_english_age": 0,
    "crows_pairs_french_socioeconomic": 0,
    "crows_pairs_english_religion": 0,
    "crows_pairs_english_gender": 0,
    "crows_pairs_english_physical_appearance": 0,
    "crows_pairs_french_nationality": 0,
    "crows_pairs_english": 0,
    "crows_pairs_french_physical_appearance": 0,
    "crows_pairs_french_gender": 0,
    "crows_pairs_french": 0,
    "crows_pairs_english_disability": 0,
    "crows_pairs_french_religion": 0,
    "crows_pairs_french_race_color": 0,
    "crows_pairs_english_sexual_orientation": 0,
    "crows_pairs_english_nationality": 0
  },
  "config": {
    "model": "hf-causal",
    "model_args": "pretrained=EleutherAI/pythia-70m-deduped,revision=step136000",
    "num_fewshot": 0,
    "batch_size": 8,
    "device": "cuda:1",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}