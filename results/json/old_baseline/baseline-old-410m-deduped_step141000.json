{
  "results": {
    "crows_pairs_french_socioeconomic": {
      "likelihood_difference": 3.659797512755102,
      "likelihood_difference_stderr": 0.2797943881353533,
      "pct_stereotype": 0.5561224489795918,
      "pct_stereotype_stderr": 0.0355794719495366
    },
    "crows_pairs_english_age": {
      "likelihood_difference": 3.190934065934066,
      "likelihood_difference_stderr": 0.31534626066486154,
      "pct_stereotype": 0.5714285714285714,
      "pct_stereotype_stderr": 0.05216405309573015
    },
    "crows_pairs_english_sexual_orientation": {
      "likelihood_difference": 4.34375,
      "likelihood_difference_stderr": 0.4638043603916702,
      "pct_stereotype": 0.7741935483870968,
      "pct_stereotype_stderr": 0.0435912209478823
    },
    "crows_pairs_french_age": {
      "likelihood_difference": 3.4524305555555554,
      "likelihood_difference_stderr": 0.3609872231336268,
      "pct_stereotype": 0.4111111111111111,
      "pct_stereotype_stderr": 0.05215564061107554
    },
    "crows_pairs_english_disability": {
      "likelihood_difference": 5.677884615384615,
      "likelihood_difference_stderr": 0.5694721495287103,
      "pct_stereotype": 0.6461538461538462,
      "pct_stereotype_stderr": 0.05977027026123099
    },
    "crows_pairs_french_disability": {
      "likelihood_difference": 5.042613636363637,
      "likelihood_difference_stderr": 0.48712790453065874,
      "pct_stereotype": 0.5757575757575758,
      "pct_stereotype_stderr": 0.06130137276858362
    },
    "crows_pairs_english_socioeconomic": {
      "likelihood_difference": 3.8108552631578947,
      "likelihood_difference_stderr": 0.23246278397805903,
      "pct_stereotype": 0.6210526315789474,
      "pct_stereotype_stderr": 0.03528765094094841
    },
    "crows_pairs_english_race_color": {
      "likelihood_difference": 3.4165538877952755,
      "likelihood_difference_stderr": 0.1605088009415095,
      "pct_stereotype": 0.452755905511811,
      "pct_stereotype_stderr": 0.022106430541228052
    },
    "crows_pairs_french": {
      "likelihood_difference": 3.789593582289803,
      "likelihood_difference_stderr": 0.0944665679528804,
      "pct_stereotype": 0.4609421586165772,
      "pct_stereotype_stderr": 0.012175979057399897
    },
    "crows_pairs_english_autre": {
      "likelihood_difference": 5.340909090909091,
      "likelihood_difference_stderr": 2.247273658152941,
      "pct_stereotype": 0.6363636363636364,
      "pct_stereotype_stderr": 0.15212000482437738
    },
    "crows_pairs_english": {
      "likelihood_difference": 3.5013323643410854,
      "likelihood_difference_stderr": 0.08890390737785821,
      "pct_stereotype": 0.5551580202742994,
      "pct_stereotype_stderr": 0.01213875607486202
    },
    "crows_pairs_french_autre": {
      "likelihood_difference": 3.9663461538461537,
      "likelihood_difference_stderr": 1.2389458212640347,
      "pct_stereotype": 0.3076923076923077,
      "pct_stereotype_stderr": 0.13323467750529824
    },
    "crows_pairs_french_race_color": {
      "likelihood_difference": 3.6654891304347825,
      "likelihood_difference_stderr": 0.19640023481911176,
      "pct_stereotype": 0.40217391304347827,
      "pct_stereotype_stderr": 0.02288695610426314
    },
    "crows_pairs_french_physical_appearance": {
      "likelihood_difference": 3.861111111111111,
      "likelihood_difference_stderr": 0.4484315269988454,
      "pct_stereotype": 0.5833333333333334,
      "pct_stereotype_stderr": 0.05850912479161746
    },
    "crows_pairs_french_nationality": {
      "likelihood_difference": 4.613142292490118,
      "likelihood_difference_stderr": 0.2566549543238249,
      "pct_stereotype": 0.31620553359683795,
      "pct_stereotype_stderr": 0.02929188048554201
    },
    "crows_pairs_english_gender": {
      "likelihood_difference": 2.961181640625,
      "likelihood_difference_stderr": 0.18600508846031877,
      "pct_stereotype": 0.56875,
      "pct_stereotype_stderr": 0.027728726065513784
    },
    "crows_pairs_english_physical_appearance": {
      "likelihood_difference": 3.5026041666666665,
      "likelihood_difference_stderr": 0.34846899046806257,
      "pct_stereotype": 0.5972222222222222,
      "pct_stereotype_stderr": 0.05820650942569533
    },
    "crows_pairs_french_gender": {
      "likelihood_difference": 3.369207554517134,
      "likelihood_difference_stderr": 0.17580340217115073,
      "pct_stereotype": 0.4953271028037383,
      "pct_stereotype_stderr": 0.027949629024360143
    },
    "crows_pairs_english_religion": {
      "likelihood_difference": 3.1554054054054053,
      "likelihood_difference_stderr": 0.3344631934799917,
      "pct_stereotype": 0.7657657657657657,
      "pct_stereotype_stderr": 0.040380976365670965
    },
    "crows_pairs_french_religion": {
      "likelihood_difference": 3.413858695652174,
      "likelihood_difference_stderr": 0.39719406692862763,
      "pct_stereotype": 0.4608695652173913,
      "pct_stereotype_stderr": 0.04668566114758416
    },
    "crows_pairs_english_nationality": {
      "likelihood_difference": 3.425419560185185,
      "likelihood_difference_stderr": 0.2546523274735869,
      "pct_stereotype": 0.46296296296296297,
      "pct_stereotype_stderr": 0.03400603625538271
    },
    "crows_pairs_french_sexual_orientation": {
      "likelihood_difference": 3.7074175824175826,
      "likelihood_difference_stderr": 0.3132311792807515,
      "pct_stereotype": 0.7252747252747253,
      "pct_stereotype_stderr": 0.04705213398778436
    }
  },
  "versions": {
    "crows_pairs_french_socioeconomic": 0,
    "crows_pairs_english_age": 0,
    "crows_pairs_english_sexual_orientation": 0,
    "crows_pairs_french_age": 0,
    "crows_pairs_english_disability": 0,
    "crows_pairs_french_disability": 0,
    "crows_pairs_english_socioeconomic": 0,
    "crows_pairs_english_race_color": 0,
    "crows_pairs_french": 0,
    "crows_pairs_english_autre": 0,
    "crows_pairs_english": 0,
    "crows_pairs_french_autre": 0,
    "crows_pairs_french_race_color": 0,
    "crows_pairs_french_physical_appearance": 0,
    "crows_pairs_french_nationality": 0,
    "crows_pairs_english_gender": 0,
    "crows_pairs_english_physical_appearance": 0,
    "crows_pairs_french_gender": 0,
    "crows_pairs_english_religion": 0,
    "crows_pairs_french_religion": 0,
    "crows_pairs_english_nationality": 0,
    "crows_pairs_french_sexual_orientation": 0
  },
  "config": {
    "model": "hf-causal",
    "model_args": "pretrained=EleutherAI/pythia-410m-deduped,revision=step141000",
    "num_fewshot": 0,
    "batch_size": 8,
    "device": "cuda:2",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}