{
  "results": {
    "crows_pairs_french_race_color": {
      "likelihood_difference": 3.705095108695652,
      "likelihood_difference_stderr": 0.19247699506833701,
      "pct_stereotype": 0.4369565217391304,
      "pct_stereotype_stderr": 0.02315174531687339
    },
    "crows_pairs_french_physical_appearance": {
      "likelihood_difference": 3.8871527777777777,
      "likelihood_difference_stderr": 0.43406562824003253,
      "pct_stereotype": 0.5555555555555556,
      "pct_stereotype_stderr": 0.05897165471491952
    },
    "crows_pairs_english": {
      "likelihood_difference": 3.5645125223613596,
      "likelihood_difference_stderr": 0.08973536922107338,
      "pct_stereotype": 0.5497912939773405,
      "pct_stereotype_stderr": 0.0121525905741749
    },
    "crows_pairs_french_religion": {
      "likelihood_difference": 3.336413043478261,
      "likelihood_difference_stderr": 0.4069650842941083,
      "pct_stereotype": 0.46956521739130436,
      "pct_stereotype_stderr": 0.046742456376794195
    },
    "crows_pairs_english_autre": {
      "likelihood_difference": 5.380681818181818,
      "likelihood_difference_stderr": 2.378256656931642,
      "pct_stereotype": 0.5454545454545454,
      "pct_stereotype_stderr": 0.1574591643244434
    },
    "crows_pairs_french_gender": {
      "likelihood_difference": 3.449133566978193,
      "likelihood_difference_stderr": 0.18011076752361124,
      "pct_stereotype": 0.48286604361370716,
      "pct_stereotype_stderr": 0.027934433698537306
    },
    "crows_pairs_french_age": {
      "likelihood_difference": 3.5524305555555555,
      "likelihood_difference_stderr": 0.3705605624547093,
      "pct_stereotype": 0.4222222222222222,
      "pct_stereotype_stderr": 0.05235473399540656
    },
    "crows_pairs_english_religion": {
      "likelihood_difference": 3.302224099099099,
      "likelihood_difference_stderr": 0.33990587147659523,
      "pct_stereotype": 0.7657657657657657,
      "pct_stereotype_stderr": 0.04038097636567097
    },
    "crows_pairs_french": {
      "likelihood_difference": 3.847015690220632,
      "likelihood_difference_stderr": 0.09403283909559713,
      "pct_stereotype": 0.4728682170542636,
      "pct_stereotype_stderr": 0.01219530472156821
    },
    "crows_pairs_french_socioeconomic": {
      "likelihood_difference": 3.8398836096938775,
      "likelihood_difference_stderr": 0.2842808154408269,
      "pct_stereotype": 0.6224489795918368,
      "pct_stereotype_stderr": 0.03471541794449721
    },
    "crows_pairs_english_sexual_orientation": {
      "likelihood_difference": 4.299731182795699,
      "likelihood_difference_stderr": 0.46092929415777295,
      "pct_stereotype": 0.7526881720430108,
      "pct_stereotype_stderr": 0.0449817218566707
    },
    "crows_pairs_french_sexual_orientation": {
      "likelihood_difference": 4.060096153846154,
      "likelihood_difference_stderr": 0.3476466452502352,
      "pct_stereotype": 0.6923076923076923,
      "pct_stereotype_stderr": 0.04865042554105199
    },
    "crows_pairs_english_socioeconomic": {
      "likelihood_difference": 3.8473684210526318,
      "likelihood_difference_stderr": 0.23144829549487575,
      "pct_stereotype": 0.631578947368421,
      "pct_stereotype_stderr": 0.03508771929824559
    },
    "crows_pairs_english_gender": {
      "likelihood_difference": 3.074755859375,
      "likelihood_difference_stderr": 0.18928811479700858,
      "pct_stereotype": 0.546875,
      "pct_stereotype_stderr": 0.027871330781745147
    },
    "crows_pairs_english_age": {
      "likelihood_difference": 3.244162087912088,
      "likelihood_difference_stderr": 0.3138166372466321,
      "pct_stereotype": 0.5824175824175825,
      "pct_stereotype_stderr": 0.05198368783767557
    },
    "crows_pairs_english_nationality": {
      "likelihood_difference": 3.4147858796296298,
      "likelihood_difference_stderr": 0.24693770414270538,
      "pct_stereotype": 0.47685185185185186,
      "pct_stereotype_stderr": 0.03406315360711507
    },
    "crows_pairs_english_disability": {
      "likelihood_difference": 5.702403846153846,
      "likelihood_difference_stderr": 0.581286471604218,
      "pct_stereotype": 0.6461538461538462,
      "pct_stereotype_stderr": 0.05977027026123099
    },
    "crows_pairs_french_nationality": {
      "likelihood_difference": 4.611413043478261,
      "likelihood_difference_stderr": 0.24716384000247812,
      "pct_stereotype": 0.31225296442687744,
      "pct_stereotype_stderr": 0.029192237133579074
    },
    "crows_pairs_english_race_color": {
      "likelihood_difference": 3.5090120570866143,
      "likelihood_difference_stderr": 0.16637608664572626,
      "pct_stereotype": 0.44291338582677164,
      "pct_stereotype_stderr": 0.022060572810922933
    },
    "crows_pairs_english_physical_appearance": {
      "likelihood_difference": 3.4876302083333335,
      "likelihood_difference_stderr": 0.3213680140262055,
      "pct_stereotype": 0.5972222222222222,
      "pct_stereotype_stderr": 0.05820650942569533
    },
    "crows_pairs_french_autre": {
      "likelihood_difference": 4.1201923076923075,
      "likelihood_difference_stderr": 1.1348937620256854,
      "pct_stereotype": 0.23076923076923078,
      "pct_stereotype_stderr": 0.12162606385262997
    },
    "crows_pairs_french_disability": {
      "likelihood_difference": 4.762310606060606,
      "likelihood_difference_stderr": 0.4786626130955138,
      "pct_stereotype": 0.5757575757575758,
      "pct_stereotype_stderr": 0.06130137276858362
    }
  },
  "versions": {
    "crows_pairs_french_race_color": 0,
    "crows_pairs_french_physical_appearance": 0,
    "crows_pairs_english": 0,
    "crows_pairs_french_religion": 0,
    "crows_pairs_english_autre": 0,
    "crows_pairs_french_gender": 0,
    "crows_pairs_french_age": 0,
    "crows_pairs_english_religion": 0,
    "crows_pairs_french": 0,
    "crows_pairs_french_socioeconomic": 0,
    "crows_pairs_english_sexual_orientation": 0,
    "crows_pairs_french_sexual_orientation": 0,
    "crows_pairs_english_socioeconomic": 0,
    "crows_pairs_english_gender": 0,
    "crows_pairs_english_age": 0,
    "crows_pairs_english_nationality": 0,
    "crows_pairs_english_disability": 0,
    "crows_pairs_french_nationality": 0,
    "crows_pairs_english_race_color": 0,
    "crows_pairs_english_physical_appearance": 0,
    "crows_pairs_french_autre": 0,
    "crows_pairs_french_disability": 0
  },
  "config": {
    "model": "hf-causal",
    "model_args": "pretrained=EleutherAI/pythia-410m-deduped,revision=step142000",
    "num_fewshot": 0,
    "batch_size": 8,
    "device": "cuda:2",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}