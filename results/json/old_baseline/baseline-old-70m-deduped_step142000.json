{
  "results": {
    "crows_pairs_english_physical_appearance": {
      "likelihood_difference": 3.3745659722222223,
      "likelihood_difference_stderr": 0.3699473843598113,
      "pct_stereotype": 0.6111111111111112,
      "pct_stereotype_stderr": 0.057855371034784615
    },
    "crows_pairs_french_gender": {
      "likelihood_difference": 4.354945482866044,
      "likelihood_difference_stderr": 0.23295911483589596,
      "pct_stereotype": 0.48598130841121495,
      "pct_stereotype_stderr": 0.02793986154930237
    },
    "crows_pairs_french_nationality": {
      "likelihood_difference": 6.9334239130434785,
      "likelihood_difference_stderr": 0.40293415228621365,
      "pct_stereotype": 0.2885375494071146,
      "pct_stereotype_stderr": 0.028541506394353756
    },
    "crows_pairs_english_race_color": {
      "likelihood_difference": 3.2691313976377954,
      "likelihood_difference_stderr": 0.1608008376269221,
      "pct_stereotype": 0.4940944881889764,
      "pct_stereotype_stderr": 0.02220423067397246
    },
    "crows_pairs_english_religion": {
      "likelihood_difference": 3.642454954954955,
      "likelihood_difference_stderr": 0.4281138347444193,
      "pct_stereotype": 0.6756756756756757,
      "pct_stereotype_stderr": 0.04463366615377136
    },
    "crows_pairs_french_religion": {
      "likelihood_difference": 4.2853260869565215,
      "likelihood_difference_stderr": 0.5106259283404012,
      "pct_stereotype": 0.4956521739130435,
      "pct_stereotype_stderr": 0.04682752006203916
    },
    "crows_pairs_english_sexual_orientation": {
      "likelihood_difference": 4.528561827956989,
      "likelihood_difference_stderr": 0.6200508700450825,
      "pct_stereotype": 0.6236559139784946,
      "pct_stereotype_stderr": 0.05050927755267201
    },
    "crows_pairs_french_sexual_orientation": {
      "likelihood_difference": 4.505837912087912,
      "likelihood_difference_stderr": 0.4132232116423431,
      "pct_stereotype": 0.6373626373626373,
      "pct_stereotype_stderr": 0.05067669921031868
    },
    "crows_pairs_english": {
      "likelihood_difference": 3.4426710644007157,
      "likelihood_difference_stderr": 0.09674966118597858,
      "pct_stereotype": 0.5396541443053071,
      "pct_stereotype_stderr": 0.012174828997623368
    },
    "crows_pairs_french_physical_appearance": {
      "likelihood_difference": 5.59375,
      "likelihood_difference_stderr": 0.7372788188529931,
      "pct_stereotype": 0.5555555555555556,
      "pct_stereotype_stderr": 0.05897165471491953
    },
    "crows_pairs_french_socioeconomic": {
      "likelihood_difference": 4.825733418367347,
      "likelihood_difference_stderr": 0.36510483974066515,
      "pct_stereotype": 0.42857142857142855,
      "pct_stereotype_stderr": 0.0354384955969167
    },
    "crows_pairs_english_nationality": {
      "likelihood_difference": 3.6263020833333335,
      "likelihood_difference_stderr": 0.2684924577129926,
      "pct_stereotype": 0.4722222222222222,
      "pct_stereotype_stderr": 0.0340470532865388
    },
    "crows_pairs_french_autre": {
      "likelihood_difference": 5.4110576923076925,
      "likelihood_difference_stderr": 1.6065616207631663,
      "pct_stereotype": 0.46153846153846156,
      "pct_stereotype_stderr": 0.14390989949130545
    },
    "crows_pairs_french": {
      "likelihood_difference": 4.959917262969588,
      "likelihood_difference_stderr": 0.12678139100772207,
      "pct_stereotype": 0.45796064400715564,
      "pct_stereotype_stderr": 0.012170053344890805
    },
    "crows_pairs_french_age": {
      "likelihood_difference": 4.283680555555556,
      "likelihood_difference_stderr": 0.41852850290124577,
      "pct_stereotype": 0.5,
      "pct_stereotype_stderr": 0.052999894000318
    },
    "crows_pairs_english_age": {
      "likelihood_difference": 2.701923076923077,
      "likelihood_difference_stderr": 0.30051114010128516,
      "pct_stereotype": 0.5824175824175825,
      "pct_stereotype_stderr": 0.051983687837675575
    },
    "crows_pairs_english_gender": {
      "likelihood_difference": 2.865966796875,
      "likelihood_difference_stderr": 0.22006101981723497,
      "pct_stereotype": 0.503125,
      "pct_stereotype_stderr": 0.027994078772422822
    },
    "crows_pairs_french_race_color": {
      "likelihood_difference": 4.468002717391304,
      "likelihood_difference_stderr": 0.22873314449064172,
      "pct_stereotype": 0.4717391304347826,
      "pct_stereotype_stderr": 0.02330069254075745
    },
    "crows_pairs_english_autre": {
      "likelihood_difference": 4.002840909090909,
      "likelihood_difference_stderr": 1.7737504331707867,
      "pct_stereotype": 0.45454545454545453,
      "pct_stereotype_stderr": 0.15745916432444335
    },
    "crows_pairs_french_disability": {
      "likelihood_difference": 6.1074810606060606,
      "likelihood_difference_stderr": 0.7258501819687462,
      "pct_stereotype": 0.48484848484848486,
      "pct_stereotype_stderr": 0.06198888629778894
    },
    "crows_pairs_english_disability": {
      "likelihood_difference": 5.2615384615384615,
      "likelihood_difference_stderr": 0.6141243398160753,
      "pct_stereotype": 0.5538461538461539,
      "pct_stereotype_stderr": 0.06213651700539812
    },
    "crows_pairs_english_socioeconomic": {
      "likelihood_difference": 3.746875,
      "likelihood_difference_stderr": 0.232865427690542,
      "pct_stereotype": 0.631578947368421,
      "pct_stereotype_stderr": 0.03508771929824559
    }
  },
  "versions": {
    "crows_pairs_english_physical_appearance": 0,
    "crows_pairs_french_gender": 0,
    "crows_pairs_french_nationality": 0,
    "crows_pairs_english_race_color": 0,
    "crows_pairs_english_religion": 0,
    "crows_pairs_french_religion": 0,
    "crows_pairs_english_sexual_orientation": 0,
    "crows_pairs_french_sexual_orientation": 0,
    "crows_pairs_english": 0,
    "crows_pairs_french_physical_appearance": 0,
    "crows_pairs_french_socioeconomic": 0,
    "crows_pairs_english_nationality": 0,
    "crows_pairs_french_autre": 0,
    "crows_pairs_french": 0,
    "crows_pairs_french_age": 0,
    "crows_pairs_english_age": 0,
    "crows_pairs_english_gender": 0,
    "crows_pairs_french_race_color": 0,
    "crows_pairs_english_autre": 0,
    "crows_pairs_french_disability": 0,
    "crows_pairs_english_disability": 0,
    "crows_pairs_english_socioeconomic": 0
  },
  "config": {
    "model": "hf-causal",
    "model_args": "pretrained=EleutherAI/pythia-70m-deduped,revision=step142000",
    "num_fewshot": 0,
    "batch_size": 8,
    "device": "cuda:1",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}