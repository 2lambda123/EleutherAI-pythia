{
  "results": {
    "crows_pairs_french_nationality": {
      "likelihood_difference": 4.32423418972332,
      "likelihood_difference_stderr": 0.250900844961659,
      "pct_stereotype": 0.2964426877470356,
      "pct_stereotype_stderr": 0.028768673758013903
    },
    "crows_pairs_english": {
      "likelihood_difference": 3.4229837507453786,
      "likelihood_difference_stderr": 0.08320512651968816,
      "pct_stereotype": 0.616577221228384,
      "pct_stereotype_stderr": 0.011876697253175876
    },
    "crows_pairs_english_nationality": {
      "likelihood_difference": 3.263310185185185,
      "likelihood_difference_stderr": 0.2342032466620418,
      "pct_stereotype": 0.5277777777777778,
      "pct_stereotype_stderr": 0.0340470532865388
    },
    "crows_pairs_english_socioeconomic": {
      "likelihood_difference": 3.7863486842105263,
      "likelihood_difference_stderr": 0.24190827933515024,
      "pct_stereotype": 0.6789473684210526,
      "pct_stereotype_stderr": 0.03396059335824887
    },
    "crows_pairs_french_physical_appearance": {
      "likelihood_difference": 3.8901909722222223,
      "likelihood_difference_stderr": 0.5148931328873292,
      "pct_stereotype": 0.6388888888888888,
      "pct_stereotype_stderr": 0.057003814617008604
    },
    "crows_pairs_french_age": {
      "likelihood_difference": 2.843402777777778,
      "likelihood_difference_stderr": 0.2713013163829781,
      "pct_stereotype": 0.5333333333333333,
      "pct_stereotype_stderr": 0.05288198530254015
    },
    "crows_pairs_french_gender": {
      "likelihood_difference": 3.330704828660436,
      "likelihood_difference_stderr": 0.17499850229096026,
      "pct_stereotype": 0.514018691588785,
      "pct_stereotype_stderr": 0.027939861549302364
    },
    "crows_pairs_french": {
      "likelihood_difference": 3.583482409063804,
      "likelihood_difference_stderr": 0.08921222132972928,
      "pct_stereotype": 0.47465712581991654,
      "pct_stereotype_stderr": 0.01219760087155068
    },
    "crows_pairs_french_socioeconomic": {
      "likelihood_difference": 3.4888392857142856,
      "likelihood_difference_stderr": 0.26825248411983066,
      "pct_stereotype": 0.5816326530612245,
      "pct_stereotype_stderr": 0.03532530943876559
    },
    "crows_pairs_french_sexual_orientation": {
      "likelihood_difference": 3.6270604395604398,
      "likelihood_difference_stderr": 0.34677834187926193,
      "pct_stereotype": 0.7582417582417582,
      "pct_stereotype_stderr": 0.04513082148355003
    },
    "crows_pairs_english_autre": {
      "likelihood_difference": 4.559659090909091,
      "likelihood_difference_stderr": 1.6225530885952293,
      "pct_stereotype": 0.6363636363636364,
      "pct_stereotype_stderr": 0.15212000482437738
    },
    "crows_pairs_french_autre": {
      "likelihood_difference": 3.3725961538461537,
      "likelihood_difference_stderr": 0.9992889946469367,
      "pct_stereotype": 0.5384615384615384,
      "pct_stereotype_stderr": 0.14390989949130545
    },
    "crows_pairs_english_disability": {
      "likelihood_difference": 5.452884615384615,
      "likelihood_difference_stderr": 0.5298890776262972,
      "pct_stereotype": 0.7538461538461538,
      "pct_stereotype_stderr": 0.05384615384615383
    },
    "crows_pairs_english_age": {
      "likelihood_difference": 3.37603021978022,
      "likelihood_difference_stderr": 0.32834539684786784,
      "pct_stereotype": 0.6263736263736264,
      "pct_stereotype_stderr": 0.0509934316638677
    },
    "crows_pairs_english_race_color": {
      "likelihood_difference": 3.3073941929133857,
      "likelihood_difference_stderr": 0.15394336827812494,
      "pct_stereotype": 0.5275590551181102,
      "pct_stereotype_stderr": 0.022172023280100765
    },
    "crows_pairs_english_gender": {
      "likelihood_difference": 2.773828125,
      "likelihood_difference_stderr": 0.1515420840092634,
      "pct_stereotype": 0.621875,
      "pct_stereotype_stderr": 0.02715025441234715
    },
    "crows_pairs_english_religion": {
      "likelihood_difference": 3.474099099099099,
      "likelihood_difference_stderr": 0.3264876967153016,
      "pct_stereotype": 0.7567567567567568,
      "pct_stereotype_stderr": 0.04090743073860917
    },
    "crows_pairs_english_physical_appearance": {
      "likelihood_difference": 2.9809027777777777,
      "likelihood_difference_stderr": 0.27583366712014923,
      "pct_stereotype": 0.6666666666666666,
      "pct_stereotype_stderr": 0.055945423886445925
    },
    "crows_pairs_french_religion": {
      "likelihood_difference": 3.6070652173913045,
      "likelihood_difference_stderr": 0.4037903118014588,
      "pct_stereotype": 0.6782608695652174,
      "pct_stereotype_stderr": 0.04375199868936841
    },
    "crows_pairs_french_disability": {
      "likelihood_difference": 4.6647727272727275,
      "likelihood_difference_stderr": 0.48263881425056454,
      "pct_stereotype": 0.5757575757575758,
      "pct_stereotype_stderr": 0.06130137276858362
    },
    "crows_pairs_english_sexual_orientation": {
      "likelihood_difference": 4.690524193548387,
      "likelihood_difference_stderr": 0.44736661554565754,
      "pct_stereotype": 0.8494623655913979,
      "pct_stereotype_stderr": 0.03728212869390004
    },
    "crows_pairs_french_race_color": {
      "likelihood_difference": 3.325883152173913,
      "likelihood_difference_stderr": 0.16733165361339825,
      "pct_stereotype": 0.3391304347826087,
      "pct_stereotype_stderr": 0.02209708145176117
    }
  },
  "versions": {
    "crows_pairs_french_nationality": 0,
    "crows_pairs_english": 0,
    "crows_pairs_english_nationality": 0,
    "crows_pairs_english_socioeconomic": 0,
    "crows_pairs_french_physical_appearance": 0,
    "crows_pairs_french_age": 0,
    "crows_pairs_french_gender": 0,
    "crows_pairs_french": 0,
    "crows_pairs_french_socioeconomic": 0,
    "crows_pairs_french_sexual_orientation": 0,
    "crows_pairs_english_autre": 0,
    "crows_pairs_french_autre": 0,
    "crows_pairs_english_disability": 0,
    "crows_pairs_english_age": 0,
    "crows_pairs_english_race_color": 0,
    "crows_pairs_english_gender": 0,
    "crows_pairs_english_religion": 0,
    "crows_pairs_english_physical_appearance": 0,
    "crows_pairs_french_religion": 0,
    "crows_pairs_french_disability": 0,
    "crows_pairs_english_sexual_orientation": 0,
    "crows_pairs_french_race_color": 0
  },
  "config": {
    "model": "hf-causal",
    "model_args": "pretrained=EleutherAI/pythia-1.4b-deduped,revision=step132000",
    "num_fewshot": 0,
    "batch_size": 4,
    "device": "cuda:3",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}