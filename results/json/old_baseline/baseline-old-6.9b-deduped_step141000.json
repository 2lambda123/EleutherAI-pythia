{
  "results": {
    "crows_pairs_english_religion": {
      "likelihood_difference": 3.807713963963964,
      "likelihood_difference_stderr": 0.34930055634942836,
      "pct_stereotype": 0.7837837837837838,
      "pct_stereotype_stderr": 0.039250566187156444
    },
    "crows_pairs_french": {
      "likelihood_difference": 3.597267255515802,
      "likelihood_difference_stderr": 0.08820816722761271,
      "pct_stereotype": 0.518783542039356,
      "pct_stereotype_stderr": 0.012204677947890614
    },
    "crows_pairs_french_race_color": {
      "likelihood_difference": 3.3547554347826085,
      "likelihood_difference_stderr": 0.1882922424802696,
      "pct_stereotype": 0.4043478260869565,
      "pct_stereotype_stderr": 0.022906966414077277
    },
    "crows_pairs_english_disability": {
      "likelihood_difference": 5.570192307692308,
      "likelihood_difference_stderr": 0.5848575908848771,
      "pct_stereotype": 0.7230769230769231,
      "pct_stereotype_stderr": 0.055934767585573
    },
    "crows_pairs_english_gender": {
      "likelihood_difference": 4.22060546875,
      "likelihood_difference_stderr": 0.20704842440083185,
      "pct_stereotype": 0.628125,
      "pct_stereotype_stderr": 0.02705990013900487
    },
    "crows_pairs_french_physical_appearance": {
      "likelihood_difference": 3.3940972222222223,
      "likelihood_difference_stderr": 0.4426869970156898,
      "pct_stereotype": 0.6388888888888888,
      "pct_stereotype_stderr": 0.0570038146170086
    },
    "crows_pairs_french_autre": {
      "likelihood_difference": 2.5072115384615383,
      "likelihood_difference_stderr": 0.6824658515255696,
      "pct_stereotype": 0.6153846153846154,
      "pct_stereotype_stderr": 0.14044168141158106
    },
    "crows_pairs_english_nationality": {
      "likelihood_difference": 3.693865740740741,
      "likelihood_difference_stderr": 0.2559658836577949,
      "pct_stereotype": 0.5694444444444444,
      "pct_stereotype_stderr": 0.03376922151252335
    },
    "crows_pairs_english_autre": {
      "likelihood_difference": 4.565340909090909,
      "likelihood_difference_stderr": 1.7251659924326936,
      "pct_stereotype": 0.6363636363636364,
      "pct_stereotype_stderr": 0.15212000482437738
    },
    "crows_pairs_french_age": {
      "likelihood_difference": 2.7961805555555554,
      "likelihood_difference_stderr": 0.2851872026292636,
      "pct_stereotype": 0.4777777777777778,
      "pct_stereotype_stderr": 0.05294752255076824
    },
    "crows_pairs_french_religion": {
      "likelihood_difference": 3.7980978260869565,
      "likelihood_difference_stderr": 0.3179699307274686,
      "pct_stereotype": 0.6956521739130435,
      "pct_stereotype_stderr": 0.043095185024639285
    },
    "crows_pairs_english_physical_appearance": {
      "likelihood_difference": 3.7300347222222223,
      "likelihood_difference_stderr": 0.35655139448695433,
      "pct_stereotype": 0.75,
      "pct_stereotype_stderr": 0.051389153237064875
    },
    "crows_pairs_french_nationality": {
      "likelihood_difference": 3.9829545454545454,
      "likelihood_difference_stderr": 0.22163845481887742,
      "pct_stereotype": 0.34782608695652173,
      "pct_stereotype_stderr": 0.030002850406189333
    },
    "crows_pairs_french_disability": {
      "likelihood_difference": 4.99905303030303,
      "likelihood_difference_stderr": 0.48341190236881837,
      "pct_stereotype": 0.6666666666666666,
      "pct_stereotype_stderr": 0.0584705346204686
    },
    "crows_pairs_french_sexual_orientation": {
      "likelihood_difference": 3.35989010989011,
      "likelihood_difference_stderr": 0.2950271489858482,
      "pct_stereotype": 0.6923076923076923,
      "pct_stereotype_stderr": 0.04865042554105198
    },
    "crows_pairs_english_sexual_orientation": {
      "likelihood_difference": 4.477486559139785,
      "likelihood_difference_stderr": 0.45363375060313443,
      "pct_stereotype": 0.8387096774193549,
      "pct_stereotype_stderr": 0.03834564688497144
    },
    "crows_pairs_english_race_color": {
      "likelihood_difference": 3.58470718503937,
      "likelihood_difference_stderr": 0.15473331257130796,
      "pct_stereotype": 0.562992125984252,
      "pct_stereotype_stderr": 0.022028849296085076
    },
    "crows_pairs_french_socioeconomic": {
      "likelihood_difference": 3.7451769770408165,
      "likelihood_difference_stderr": 0.26631207378446226,
      "pct_stereotype": 0.7193877551020408,
      "pct_stereotype_stderr": 0.03217492357780148
    },
    "crows_pairs_english_socioeconomic": {
      "likelihood_difference": 4.2261513157894735,
      "likelihood_difference_stderr": 0.24503988120652737,
      "pct_stereotype": 0.6894736842105263,
      "pct_stereotype_stderr": 0.03365713545671698
    },
    "crows_pairs_english_age": {
      "likelihood_difference": 3.799107142857143,
      "likelihood_difference_stderr": 0.363024251077407,
      "pct_stereotype": 0.6263736263736264,
      "pct_stereotype_stderr": 0.0509934316638677
    },
    "crows_pairs_english": {
      "likelihood_difference": 3.9583146988670244,
      "likelihood_difference_stderr": 0.09006601754247379,
      "pct_stereotype": 0.6386404293381037,
      "pct_stereotype_stderr": 0.011734402417305046
    },
    "crows_pairs_french_gender": {
      "likelihood_difference": 3.5719431464174454,
      "likelihood_difference_stderr": 0.1842950717314883,
      "pct_stereotype": 0.5327102803738317,
      "pct_stereotype_stderr": 0.027890972865217988
    }
  },
  "versions": {
    "crows_pairs_english_religion": 0,
    "crows_pairs_french": 0,
    "crows_pairs_french_race_color": 0,
    "crows_pairs_english_disability": 0,
    "crows_pairs_english_gender": 0,
    "crows_pairs_french_physical_appearance": 0,
    "crows_pairs_french_autre": 0,
    "crows_pairs_english_nationality": 0,
    "crows_pairs_english_autre": 0,
    "crows_pairs_french_age": 0,
    "crows_pairs_french_religion": 0,
    "crows_pairs_english_physical_appearance": 0,
    "crows_pairs_french_nationality": 0,
    "crows_pairs_french_disability": 0,
    "crows_pairs_french_sexual_orientation": 0,
    "crows_pairs_english_sexual_orientation": 0,
    "crows_pairs_english_race_color": 0,
    "crows_pairs_french_socioeconomic": 0,
    "crows_pairs_english_socioeconomic": 0,
    "crows_pairs_english_age": 0,
    "crows_pairs_english": 0,
    "crows_pairs_french_gender": 0
  },
  "config": {
    "model": "hf-causal",
    "model_args": "use_accelerate=True,pretrained=EleutherAI/pythia-6.9b-deduped,revision=step141000",
    "num_fewshot": 0,
    "batch_size": 2,
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}