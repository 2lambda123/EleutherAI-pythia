{
  "results": {
    "crows_pairs_english_physical_appearance": {
      "likelihood_difference": 3.189670138888889,
      "likelihood_difference_stderr": 0.283816191135745,
      "pct_stereotype": 0.6805555555555556,
      "pct_stereotype_stderr": 0.05533504751887218
    },
    "crows_pairs_french_age": {
      "likelihood_difference": 2.9854166666666666,
      "likelihood_difference_stderr": 0.28293045960983276,
      "pct_stereotype": 0.4666666666666667,
      "pct_stereotype_stderr": 0.05288198530254015
    },
    "crows_pairs_english_nationality": {
      "likelihood_difference": 3.2861689814814814,
      "likelihood_difference_stderr": 0.23344393410291953,
      "pct_stereotype": 0.5,
      "pct_stereotype_stderr": 0.034099716973523674
    },
    "crows_pairs_english_religion": {
      "likelihood_difference": 3.4510135135135136,
      "likelihood_difference_stderr": 0.3251148617010813,
      "pct_stereotype": 0.7297297297297297,
      "pct_stereotype_stderr": 0.042343213610845386
    },
    "crows_pairs_french_sexual_orientation": {
      "likelihood_difference": 3.7208104395604398,
      "likelihood_difference_stderr": 0.339076976115594,
      "pct_stereotype": 0.7362637362637363,
      "pct_stereotype_stderr": 0.04644942852497395
    },
    "crows_pairs_english_sexual_orientation": {
      "likelihood_difference": 4.56619623655914,
      "likelihood_difference_stderr": 0.46047854754746587,
      "pct_stereotype": 0.8494623655913979,
      "pct_stereotype_stderr": 0.03728212869390004
    },
    "crows_pairs_french_physical_appearance": {
      "likelihood_difference": 3.8832465277777777,
      "likelihood_difference_stderr": 0.511512471473122,
      "pct_stereotype": 0.6111111111111112,
      "pct_stereotype_stderr": 0.05785537103478461
    },
    "crows_pairs_french_socioeconomic": {
      "likelihood_difference": 3.54671556122449,
      "likelihood_difference_stderr": 0.26786863694899316,
      "pct_stereotype": 0.5867346938775511,
      "pct_stereotype_stderr": 0.035262902194360866
    },
    "crows_pairs_english_age": {
      "likelihood_difference": 3.493818681318681,
      "likelihood_difference_stderr": 0.3482169047159315,
      "pct_stereotype": 0.6373626373626373,
      "pct_stereotype_stderr": 0.05067669921031868
    },
    "crows_pairs_english_socioeconomic": {
      "likelihood_difference": 3.725986842105263,
      "likelihood_difference_stderr": 0.24224921878254801,
      "pct_stereotype": 0.6052631578947368,
      "pct_stereotype_stderr": 0.035554538744639326
    },
    "crows_pairs_english": {
      "likelihood_difference": 3.4349563953488373,
      "likelihood_difference_stderr": 0.08455955650874544,
      "pct_stereotype": 0.6040548598688134,
      "pct_stereotype_stderr": 0.011945894985449665
    },
    "crows_pairs_french_race_color": {
      "likelihood_difference": 3.383491847826087,
      "likelihood_difference_stderr": 0.17009446812222206,
      "pct_stereotype": 0.3347826086956522,
      "pct_stereotype_stderr": 0.02202707848343573
    },
    "crows_pairs_french_autre": {
      "likelihood_difference": 3.8341346153846154,
      "likelihood_difference_stderr": 1.096667266878654,
      "pct_stereotype": 0.38461538461538464,
      "pct_stereotype_stderr": 0.1404416814115811
    },
    "crows_pairs_french_religion": {
      "likelihood_difference": 3.493478260869565,
      "likelihood_difference_stderr": 0.39058375938900475,
      "pct_stereotype": 0.6347826086956522,
      "pct_stereotype_stderr": 0.04509577025262067
    },
    "crows_pairs_english_autre": {
      "likelihood_difference": 4.369318181818182,
      "likelihood_difference_stderr": 1.7505671675368102,
      "pct_stereotype": 0.7272727272727273,
      "pct_stereotype_stderr": 0.14083575804390605
    },
    "crows_pairs_french_disability": {
      "likelihood_difference": 4.6647727272727275,
      "likelihood_difference_stderr": 0.47489635179822226,
      "pct_stereotype": 0.5303030303030303,
      "pct_stereotype_stderr": 0.06190336468479955
    },
    "crows_pairs_french_nationality": {
      "likelihood_difference": 4.378458498023716,
      "likelihood_difference_stderr": 0.25485331490545376,
      "pct_stereotype": 0.30039525691699603,
      "pct_stereotype_stderr": 0.028878367428103884
    },
    "crows_pairs_french_gender": {
      "likelihood_difference": 3.385514018691589,
      "likelihood_difference_stderr": 0.1734671762222126,
      "pct_stereotype": 0.5015576323987538,
      "pct_stereotype_stderr": 0.027950714088670354
    },
    "crows_pairs_english_disability": {
      "likelihood_difference": 5.629326923076923,
      "likelihood_difference_stderr": 0.5577843352866754,
      "pct_stereotype": 0.7384615384615385,
      "pct_stereotype_stderr": 0.05493406483494501
    },
    "crows_pairs_english_gender": {
      "likelihood_difference": 2.7171875,
      "likelihood_difference_stderr": 0.15632754553503342,
      "pct_stereotype": 0.634375,
      "pct_stereotype_stderr": 0.026964702306061943
    },
    "crows_pairs_english_race_color": {
      "likelihood_difference": 3.354115403543307,
      "likelihood_difference_stderr": 0.15500090106668918,
      "pct_stereotype": 0.5196850393700787,
      "pct_stereotype_stderr": 0.022188563396746387
    },
    "crows_pairs_french": {
      "likelihood_difference": 3.632919648181276,
      "likelihood_difference_stderr": 0.08940224411979689,
      "pct_stereotype": 0.4603458556946929,
      "pct_stereotype_stderr": 0.012174828997623368
    }
  },
  "versions": {
    "crows_pairs_english_physical_appearance": 0,
    "crows_pairs_french_age": 0,
    "crows_pairs_english_nationality": 0,
    "crows_pairs_english_religion": 0,
    "crows_pairs_french_sexual_orientation": 0,
    "crows_pairs_english_sexual_orientation": 0,
    "crows_pairs_french_physical_appearance": 0,
    "crows_pairs_french_socioeconomic": 0,
    "crows_pairs_english_age": 0,
    "crows_pairs_english_socioeconomic": 0,
    "crows_pairs_english": 0,
    "crows_pairs_french_race_color": 0,
    "crows_pairs_french_autre": 0,
    "crows_pairs_french_religion": 0,
    "crows_pairs_english_autre": 0,
    "crows_pairs_french_disability": 0,
    "crows_pairs_french_nationality": 0,
    "crows_pairs_french_gender": 0,
    "crows_pairs_english_disability": 0,
    "crows_pairs_english_gender": 0,
    "crows_pairs_english_race_color": 0,
    "crows_pairs_french": 0
  },
  "config": {
    "model": "hf-causal",
    "model_args": "pretrained=EleutherAI/pythia-1.4b-deduped,revision=step137000",
    "num_fewshot": 0,
    "batch_size": 4,
    "device": "cuda:3",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}