train/lm_loss	step
11.013898849487305	1
11.016687393188477	2
11.014725685119629	3
11.014728546142578	4
11.01385498046875	5
11.01066780090332	6
11.006002426147461	7
11.007051467895508	8
10.995844841003418	9
10.988714218139648	10
10.98495101928711	11
10.973173141479492	12
10.962503433227539	13
10.954360961914062	14
10.945194244384766	15
10.939308166503906	16
10.927250862121582	17
10.912090301513672	18
10.898796081542969	19
10.884897232055664	20
10.879023551940918	21
10.859177589416504	22
10.845649719238281	23
10.833011627197266	24
10.818520545959473	25
10.802289009094238	26
10.786547660827637	27
10.768654823303223	28
10.756477355957031	29
10.737329483032227	30
10.725484848022461	31
10.706063270568848	32
10.695074081420898	33
10.68027114868164	34
10.662271499633789	35
10.652438163757324	36
10.637288093566895	37
10.622864723205566	38
10.607778549194336	39
10.589534759521484	40
10.577512741088867	41
10.560686111450195	42
10.555351257324219	43
10.538993835449219	44
10.522530555725098	45
10.51749038696289	46
10.498620986938477	47
10.481816291809082	48
10.476818084716797	49
10.464475631713867	50
10.452055931091309	51
10.438852310180664	52
10.426523208618164	53
10.411320686340332	54
10.404104232788086	55
10.4026460647583	56
10.38219928741455	57
10.36878776550293	58
10.365777969360352	59
10.356736183166504	60
10.336199760437012	61
10.328018188476562	62
10.312896728515625	63
