train/lm_loss	step
2.203000068664551	122001
10.984621047973633	122002
10.086402893066406	122003
9.773192405700684	122004
9.751718521118164	122005
9.6493501663208	122006
9.58767032623291	122007
9.517887115478516	122008
9.502311706542969	122009
9.452286720275879	122010
9.411484718322754	122011
9.37595272064209	122012
9.328314781188965	122013
9.271795272827148	122014
9.221977233886719	122015
9.153583526611328	122016
9.124597549438477	122017
9.096317291259766	122018
9.020124435424805	122019
8.981977462768555	122020
8.931718826293945	122021
8.921743392944336	122022
8.873602867126465	122023
8.865632057189941	122024
8.792851448059082	122025
8.75229263305664	122026
8.729558944702148	122027
8.68821907043457	122028
8.640870094299316	122029
8.63037109375	122030
8.590489387512207	122031
8.553771018981934	122032
8.49479866027832	122033
8.48495101928711	122034
8.409759521484375	122035
8.422327995300293	122036
8.394977569580078	122037
8.346946716308594	122038
8.299839973449707	122039
8.287732124328613	122040
8.247028350830078	122041
8.192602157592773	122042
8.139171600341797	122043
8.141973495483398	122044
8.092710494995117	122045
8.067182540893555	122046
8.036864280700684	122047
7.981494903564453	122048
7.969818592071533	122049
