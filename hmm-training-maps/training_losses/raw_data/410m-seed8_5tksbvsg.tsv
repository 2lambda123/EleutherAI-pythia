train/lm_loss	step
2.2107737064361572	75001
2.214402198791504	75002
2.2155041694641113	75003
2.274120330810547	75004
2.2495195865631104	75005
2.222832202911377	75006
2.2038910388946533	75007
2.2330875396728516	75008
2.256275177001953	75009
2.2553369998931885	75010
2.2070159912109375	75011
2.21193528175354	75012
2.1993045806884766	75013
2.233506202697754	75014
2.2715654373168945	75015
2.251781940460205	75016
2.2388854026794434	75017
2.2345595359802246	75018
2.2117559909820557	75019
2.2073616981506348	75020
2.250631809234619	75021
2.224673271179199	75022
2.180236339569092	75023
2.2060775756835938	75024
2.2272512912750244	75025
2.232882022857666	75026
2.1763932704925537	75027
2.2133424282073975	75028
2.2279505729675293	75029
2.230480194091797	75030
2.233015775680542	75031
2.212531566619873	75032
2.195324420928955	75033
2.220221519470215	75034
2.2649879455566406	75035
2.201666831970215	75036
2.2391133308410645	75037
2.2501606941223145	75038
2.20776629447937	75039
2.2386083602905273	75040
2.2529640197753906	75041
2.1871650218963623	75042
2.2432451248168945	75043
2.1994357109069824	75044
2.1768746376037598	75045
2.21632719039917	75046
2.196591854095459	75047
2.2446091175079346	75048
2.2633485794067383	75049
2.223735809326172	75050
2.2008934020996094	75051
2.2358906269073486	75052
2.2433009147644043	75053
2.2181077003479004	75054
2.211907386779785	75055
2.2357568740844727	75056
2.247661590576172	75057
2.2326698303222656	75058
2.2266149520874023	75059
2.2084784507751465	75060
2.228476047515869	75061
2.2159810066223145	75062
2.2201011180877686	75063
2.232595682144165	75064
2.2313156127929688	75065
2.246295928955078	75066
2.2177987098693848	75067
2.229680299758911	75068
2.196385383605957	75069
2.2005035877227783	75070
2.2106783390045166	75071
2.1614162921905518	75072
2.2118945121765137	75073
2.1996822357177734	75074
2.174598217010498	75075
2.1911680698394775	75076
2.252582550048828	75077
2.230450391769409	75078
2.2243781089782715	75079
